import torchimport torch.nn as nnimport torch.optim as optimimport randomimport numpy as npfrom collections import dequeimport pickleclass QNetwork(nn.Module):    def __init__(self, state_size, action_size):        super(QNetwork, self).__init__()        self.fc1 = nn.Linear(state_size, 32)        self.fc2 = nn.Linear(32, 64)        self.fc3 = nn.Linear(64, action_size)    def forward(self, x):        x = torch.relu(self.fc1(x))        x = torch.relu(self.fc2(x))        return self.fc3(x)class DeepIHRLAgent:    def __init__(self, state_size, action_size):        self.state_size = state_size        self.action_size = action_size        self.model = QNetwork(state_size, action_size)        self.target_model = QNetwork(state_size, action_size)        self.update_target_model()        self.memory = deque(maxlen=5000)        self.gamma = 0.95        self.epsilon = 1.0        self.epsilon_decay = 0.98        self.epsilon_min = 0.00                self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)        self.loss_fn = nn.MSELoss()    def save(self, path="dqn_model.pth", memory_path="memory.pkl"):        torch.save(self.model.state_dict(), path)        with open(memory_path, "wb") as f:            pickle.dump(self.memory, f)        def load(self, path="dqn_model.pth", memory_path="memory.pkl"):        self.model.load_state_dict(torch.load(path))        self.update_target_model()        try:            with open(memory_path, "rb") as f:                self.memory = pickle.load(f)        except FileNotFoundError:            print("No memory file found. Starting with empty memory.")    def save_hyperparams(self, path="params.pkl"):        with open(path, "wb") as f:            pickle.dump({                "epsilon": self.epsilon,                "epsilon_decay": self.epsilon_decay,                "epsilon_min": self.epsilon_min            }, f)        def load_hyperparams(self, path="params.pkl"):        with open(path, "rb") as f:            params = pickle.load(f)            self.epsilon = params["epsilon"]            self.epsilon_decay = params["epsilon_decay"]            self.epsilon_min = params["epsilon_min"]    def update_target_model(self):        self.target_model.load_state_dict(self.model.state_dict())    def remember(self, state, action, reward, next_state, done):        self.memory.append((state, action, reward, next_state, done))    def get_action(self, state):        if np.random.rand() <= self.epsilon:            return random.randrange(self.action_size)        state_tensor = torch.FloatTensor(state).unsqueeze(0)        with torch.no_grad():            q_values = self.model(state_tensor)        return torch.argmax(q_values).item()        def replay(self, batch_size):        if len(self.memory) < batch_size:            return        minibatch = random.sample(self.memory, batch_size)        for state, action, reward, next_state, done in minibatch:            state_tensor = torch.FloatTensor(state).unsqueeze(0)            target = self.model(state_tensor).clone().detach()            if done or next_state is None:                target[0][action] = reward            else:                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)                with torch.no_grad():                    t = self.target_model(next_state_tensor)                target[0][action] = reward + self.gamma * torch.max(t)            output = self.model(state_tensor)            loss = self.loss_fn(output, target)            self.optimizer.zero_grad()            loss.backward()            self.optimizer.step()        # Exploration decay        if self.epsilon > self.epsilon_min:            self.epsilon *= self.epsilon_decay